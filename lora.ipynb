{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Implementation with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the model deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a network to classift MNIST digits then fine-tune the network on a specific digit in which it did not learn well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download= True, transform= transform)\n",
    "\n",
    "# Create a dataloader for training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load MNIST Test set\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Device ---Check to see if possible to train on gpu instead of CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Metal Performance Shaders\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network to classify digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyDigits(nn.Module):\n",
    "    def __init__(self, hidden_size_1= 1000, hidden_size_2= 2000):\n",
    "        super(ClassifyDigits, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "    \n",
    "        return x\n",
    "        \n",
    "net = ClassifyDigits().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network for only 1 EPOCH to simulate complete pre-training on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr= 0.001)\n",
    "    \n",
    "    total_iterations = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        \n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "        \n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "            \n",
    "        for data in data_iterator:\n",
    "            num_iterations +=1 \n",
    "            total_iterations +=1\n",
    "            \n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = net(x)\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss= avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:38<00:00, 156.97it/s, loss=0.238]\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, net, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a copy of the original weights to prove fine-tuning with LoRA doesn't impact original weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {}\n",
    "for name, param, in net.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performence of the pretrained network. The network performs poorly on digit 9. Let's fine tune on digit 9 to improve performence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Test'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = net(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] += 1\n",
    "                total += 1\n",
    "    \n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1000/1000 [00:04<00:00, 222.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.955\n",
      "wrong counts for the digit 0: 36\n",
      "wrong counts for the digit 1: 23\n",
      "wrong counts for the digit 2: 48\n",
      "wrong counts for the digit 3: 69\n",
      "wrong counts for the digit 4: 18\n",
      "wrong counts for the digit 5: 15\n",
      "wrong counts for the digit 6: 86\n",
      "wrong counts for the digit 7: 47\n",
      "wrong counts for the digit 8: 41\n",
      "wrong counts for the digit 9: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how many parameters are in the original network, before introducing the LoRA matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
      "Total number of parameters: 2807010\n"
     ]
    }
   ],
   "source": [
    "total_parameters_original = 0\n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "\n",
    "print(f'Total number of parameters: {total_parameters_original}') ## i want to see what the output looks like first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LoRA Parameterization in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "        \"\"\"\n",
    "        LoRA Parameterization for low-rank adaptation of linear layers.\n",
    "\n",
    "        Args:\n",
    "            features_in (int): Number of input features.\n",
    "            features_out (int): Number of output features.\n",
    "            rank (int): Rank for the low-rank decomposition.\n",
    "            alpha (float): Scaling factor for the LoRA update.\n",
    "            device (str): Device to place the parameters ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize low-rank matrices A and B\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank, features_out), device=device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank), device=device))\n",
    "        \n",
    "        # Gaussian initialization for both A and B\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=0.01)  # Smaller std for stability\n",
    "        nn.init.normal_(self.lora_B, mean=0, std=0.01)\n",
    "        \n",
    "        self.scale = alpha / rank  # Scaling factor\n",
    "        self.enabled = True  # Toggle for enabling/disabling LoRA\n",
    "    \n",
    "    def forward(self, original_weights):\n",
    "        \"\"\"\n",
    "        Applies the LoRA parameterization to the original weights.\n",
    "\n",
    "        Args:\n",
    "            original_weights (Tensor): The original weights of the layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Updated weights with LoRA adjustments.\n",
    "        \"\"\"\n",
    "        if self.enabled:\n",
    "            # Compute LoRA update and add to original weights\n",
    "            lora_update = torch.matmul(self.lora_B, self.lora_A)  # (features_in, features_out)\n",
    "            return original_weights + lora_update.view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add parameterization to out network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    \n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=2000, out_features=10, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoRAParametrization()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametrize.register_parametrization(\n",
    "    net.linear1, 'weight', linear_layer_parameterization(net.linear1, device)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear2, 'weight', linear_layer_parameterization(net.linear2, device)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear3, 'weight', linear_layer_parameterization(net.linear3, device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
    "        # Corrected 'parametrizations' spelling\n",
    "        layer.parametrizations['weight'][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_lora += (\n",
    "        layer.parametrizations['weight'][0].lora_A.nelement() \n",
    "        + layer.parametrizations['weight'][0].lora_B.nelement()\n",
    "    )\n",
    "    \n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    \n",
    "    print(\n",
    "        f\"Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + \"\n",
    "        f\"Lora_A: {layer.parametrizations['weight'][0].lora_A.shape} + \"\n",
    "        f\"Lora_B: {layer.parametrizations['weight'][0].lora_B.shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters (original): 2,807,010\n",
      "Total number of parameters (original + LoRA): 2,813,804\n",
      "Parameters introduced by LoRA: 6,794\n",
      "Parameters incremenet: 0.242%\n"
     ]
    }
   ],
   "source": [
    "assert total_parameters_non_lora == total_parameters_original\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_increment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters incremenet: {parameters_increment:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze all non-LoRA parameters. Fine-tune on digit 9 for 100 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-lora parameters linear1.bias\n",
      "Freezing non-lora parameters linear1.parametrizations.weight.original\n",
      "Freezing non-lora parameters linear2.bias\n",
      "Freezing non-lora parameters linear2.parametrizations.weight.original\n",
      "Freezing non-lora parameters linear3.bias\n",
      "Freezing non-lora parameters linear3.parametrizations.weight.original\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-lora parameters {name}')\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:02<00:00, 46.27it/s, loss=0.0823]\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download= True, transform= transform)\n",
    "exclude_indices = mnist_trainset.targets == 9\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "train(train_loader, net, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that fine-tuning has not altered the original weights, but only the ones introduced by LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frozen parameters are still unchanged by fine-tuning\n",
    "assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
    "assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
    "assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "# The new linear1.weights is obtained by the forward function of our LoRA parametization\n",
    "# The original weights have been moved to net.linear1.parametrizations.weight.original\n",
    "assert torch.equal(net.linear1.weight,\n",
    "                   net.linear1.parametrizations.weight.original\n",
    "                   + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A).view(net.linear1.weight.shape)\n",
    ")\n",
    "\n",
    "# If we disable LoRA, linear1.weight is the original\n",
    "enable_disable_lora(enabled=False)\n",
    "assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network with LoRA ENABLED. The digit 9 accuracy should improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1000/1000 [00:05<00:00, 192.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.929\n",
      "wrong counts for the digit 0: 68\n",
      "wrong counts for the digit 1: 34\n",
      "wrong counts for the digit 2: 65\n",
      "wrong counts for the digit 3: 88\n",
      "wrong counts for the digit 4: 91\n",
      "wrong counts for the digit 5: 20\n",
      "wrong counts for the digit 6: 112\n",
      "wrong counts for the digit 7: 100\n",
      "wrong counts for the digit 8: 114\n",
      "wrong counts for the digit 9: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with LoRA DISABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1000/1000 [00:04<00:00, 223.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.955\n",
      "wrong counts for the digit 0: 36\n",
      "wrong counts for the digit 1: 23\n",
      "wrong counts for the digit 2: 48\n",
      "wrong counts for the digit 3: 69\n",
      "wrong counts for the digit 4: 18\n",
      "wrong counts for the digit 5: 15\n",
      "wrong counts for the digit 6: 86\n",
      "wrong counts for the digit 7: 47\n",
      "wrong counts for the digit 8: 41\n",
      "wrong counts for the digit 9: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
